import { bo as flatten, dt as tensor1d, qa as tidy, ut as tensor2d, yo as fetch } from "./dist-BkE00HEr.js";
import { r as loadGraphModel } from "./dist-B0GXU563.js";

//#region node_modules/@tensorflow-models/universal-sentence-encoder/dist/universal-sentence-encoder.esm.js
/**
* @license
* Copyright 2021 Google LLC. All Rights Reserved.
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
* =============================================================================
*/
function __awaiter(e, n, t, r) {
	return new (t || (t = Promise))(function(o, i) {
		function a(e$1) {
			try {
				u(r.next(e$1));
			} catch (e$2) {
				i(e$2);
			}
		}
		function s(e$1) {
			try {
				u(r.throw(e$1));
			} catch (e$2) {
				i(e$2);
			}
		}
		function u(e$1) {
			var n$1;
			e$1.done ? o(e$1.value) : (n$1 = e$1.value, n$1 instanceof t ? n$1 : new t(function(e$2) {
				e$2(n$1);
			})).then(a, s);
		}
		u((r = r.apply(e, n || [])).next());
	});
}
function __generator(e, n) {
	var t, r, o, i, a = {
		label: 0,
		sent: function() {
			if (1 & o[0]) throw o[1];
			return o[1];
		},
		trys: [],
		ops: []
	};
	return i = {
		next: s(0),
		throw: s(1),
		return: s(2)
	}, "function" == typeof Symbol && (i[Symbol.iterator] = function() {
		return this;
	}), i;
	function s(i$1) {
		return function(s$1) {
			return function(i$2) {
				if (t) throw new TypeError("Generator is already executing.");
				for (; a;) try {
					if (t = 1, r && (o = 2 & i$2[0] ? r.return : i$2[0] ? r.throw || ((o = r.return) && o.call(r), 0) : r.next) && !(o = o.call(r, i$2[1])).done) return o;
					switch (r = 0, o && (i$2 = [2 & i$2[0], o.value]), i$2[0]) {
						case 0:
						case 1:
							o = i$2;
							break;
						case 4: return a.label++, {
							value: i$2[1],
							done: !1
						};
						case 5:
							a.label++, r = i$2[1], i$2 = [0];
							continue;
						case 7:
							i$2 = a.ops.pop(), a.trys.pop();
							continue;
						default:
							if (!(o = (o = a.trys).length > 0 && o[o.length - 1]) && (6 === i$2[0] || 2 === i$2[0])) {
								a = 0;
								continue;
							}
							if (3 === i$2[0] && (!o || i$2[1] > o[0] && i$2[1] < o[3])) {
								a.label = i$2[1];
								break;
							}
							if (6 === i$2[0] && a.label < o[1]) {
								a.label = o[1], o = i$2;
								break;
							}
							if (o && a.label < o[2]) {
								a.label = o[2], a.ops.push(i$2);
								break;
							}
							o[2] && a.ops.pop(), a.trys.pop();
							continue;
					}
					i$2 = n.call(e, a);
				} catch (e$1) {
					i$2 = [6, e$1], r = 0;
				} finally {
					t = o = 0;
				}
				if (5 & i$2[0]) throw i$2[1];
				return {
					value: i$2[0] ? i$2[1] : void 0,
					done: !0
				};
			}([i$1, s$1]);
		};
	}
}
var stringToChars = function(e) {
	for (var n = [], t = 0, r = e; t < r.length; t++) {
		var o = r[t];
		n.push(o);
	}
	return n;
}, TrieNode = function() {
	return function() {
		this.parent = null, this.children = {}, this.end = !1, this.word = [
			[],
			0,
			0
		];
	};
}(), Trie = function() {
	function e() {
		this.root = new TrieNode();
	}
	return e.prototype.insert = function(e$1, n, t) {
		for (var r = this.root, o = stringToChars(e$1), i = 0; i < o.length; i++) r.children[o[i]] || (r.children[o[i]] = new TrieNode(), r.children[o[i]].parent = r, r.children[o[i]].word[0] = r.word[0].concat(o[i])), r = r.children[o[i]], i === o.length - 1 && (r.end = !0, r.word[1] = n, r.word[2] = t);
	}, e.prototype.commonPrefixSearch = function(e$1) {
		for (var n = [], t = this.root.children[e$1[0]], r = 0; r < e$1.length && t; r++) t.end && n.push(t.word), t = t.children[e$1[r + 1]];
		return n.length || n.push([
			[e$1[0]],
			0,
			0
		]), n;
	}, e;
}(), separator = "â–";
function processInput(e) {
	var n = e.normalize("NFKC");
	return n.length > 0 ? separator + n.replace(/ /g, separator) : n;
}
var RESERVED_SYMBOLS_COUNT = 6, Tokenizer = function() {
	function e(e$1, n) {
		void 0 === n && (n = RESERVED_SYMBOLS_COUNT), this.vocabulary = e$1, this.reservedSymbolsCount = n, this.trie = new Trie();
		for (var t = this.reservedSymbolsCount; t < this.vocabulary.length; t++) this.trie.insert(this.vocabulary[t][0], this.vocabulary[t][1], t);
	}
	return e.prototype.encode = function(e$1) {
		var n = [], t = [], r = [];
		e$1 = processInput(e$1);
		for (var o = stringToChars(e$1), i = 0; i <= o.length; i++) n.push({}), t.push(0), r.push(0);
		for (i = 0; i < o.length; i++) for (var a = this.trie.commonPrefixSearch(o.slice(i)), s = 0; s < a.length; s++) {
			var u = a[s], l = {
				key: u[0],
				score: u[1],
				index: u[2]
			};
			n[i + (c = u[0].length)][i] ?? (n[i + c][i] = []), n[i + c][i].push(l);
		}
		for (var c = 0; c <= o.length; c++) for (var h in n[c]) {
			var d = n[c][h];
			for (s = 0; s < d.length; s++) {
				var f = d[s], _ = f.score + r[c - f.key.length];
				(0 === r[c] || _ >= r[c]) && (r[c] = _, t[c] = d[s].index);
			}
		}
		for (var v = [], p = t.length - 1; p > 0;) v.push(t[p]), p -= this.vocabulary[t[p]][0].length;
		var E = [], T = !1;
		for (i = 0; i < v.length; i++) {
			var g = v[i];
			T && 0 === g || E.push(g), T = 0 === g;
		}
		return E.reverse();
	}, e;
}();
function loadTokenizer(e) {
	return __awaiter(this, void 0, void 0, function() {
		var n;
		return __generator(this, function(t) {
			switch (t.label) {
				case 0: return [4, loadVocabulary(e)];
				case 1: return n = t.sent(), [2, new Tokenizer(n)];
			}
		});
	});
}
function loadVocabulary(e) {
	return __awaiter(this, void 0, void 0, function() {
		return __generator(this, function(n) {
			switch (n.label) {
				case 0: return [4, fetch(e)];
				case 1: return [2, n.sent().json()];
			}
		});
	});
}
var version = "1.3.3", BASE_PATH = "https://tfhub.dev/google/tfjs-model/universal-sentence-encoder-qa-ondevice/1", SKIP_VALUES = [
	0,
	1,
	2
], OFFSET = 3, INPUT_LIMIT = 192, QUERY_NODE_NAME = "input_inp_text", RESPONSE_CONTEXT_NODE_NAME = "input_res_context", RESPONSE_NODE_NAME = "input_res_text", RESPONSE_RESULT_NODE_NAME = "Final/EncodeResult/mul", QUERY_RESULT_NODE_NAME = "Final/EncodeQuery/mul", RESERVED_SYMBOLS_COUNT$1 = 3, TOKEN_PADDING = 2, TOKEN_START_VALUE = 1;
function loadQnA() {
	return __awaiter(this, void 0, void 0, function() {
		var e;
		return __generator(this, function(n) {
			switch (n.label) {
				case 0: return [4, (e = new UniversalSentenceEncoderQnA()).load()];
				case 1: return n.sent(), [2, e];
			}
		});
	});
}
var UniversalSentenceEncoderQnA = function() {
	function e() {}
	return e.prototype.loadModel = function() {
		return __awaiter(this, void 0, void 0, function() {
			return __generator(this, function(e$1) {
				return [2, loadGraphModel(BASE_PATH, { fromTFHub: !0 })];
			});
		});
	}, e.prototype.load = function() {
		return __awaiter(this, void 0, void 0, function() {
			var e$1, n, t;
			return __generator(this, function(r) {
				switch (r.label) {
					case 0: return [4, Promise.all([this.loadModel(), loadVocabulary(BASE_PATH + "/vocab.json?tfjs-format=file")])];
					case 1: return e$1 = r.sent(), n = e$1[0], t = e$1[1], this.model = n, this.tokenizer = new Tokenizer(t, RESERVED_SYMBOLS_COUNT$1), [2];
				}
			});
		});
	}, e.prototype.embed = function(e$1) {
		var n = this, t = tidy(function() {
			var t$1 = n.tokenizeStrings(e$1.queries, INPUT_LIMIT), r = n.tokenizeStrings(e$1.responses, INPUT_LIMIT);
			if (null != e$1.contexts && e$1.contexts.length !== e$1.responses.length) throw new Error("The length of response strings and context strings need to match.");
			var o = e$1.contexts || [];
			e$1.contexts ?? (o.length = e$1.responses.length, o.fill(""));
			var i = n.tokenizeStrings(o, INPUT_LIMIT), a = {};
			return a[QUERY_NODE_NAME] = t$1, a[RESPONSE_NODE_NAME] = r, a[RESPONSE_CONTEXT_NODE_NAME] = i, n.model.execute(a, [QUERY_RESULT_NODE_NAME, RESPONSE_RESULT_NODE_NAME]);
		});
		return {
			queryEmbedding: t[0],
			responseEmbedding: t[1]
		};
	}, e.prototype.tokenizeStrings = function(e$1, n) {
		var t = this;
		return tensor2d(e$1.map(function(e$2) {
			return t.shiftTokens(t.tokenizer.encode(e$2), INPUT_LIMIT);
		}), [e$1.length, INPUT_LIMIT], "int32");
	}, e.prototype.shiftTokens = function(e$1, n) {
		e$1.unshift(TOKEN_START_VALUE);
		for (var t = 0; t < n; t++) t >= e$1.length ? e$1[t] = TOKEN_PADDING : SKIP_VALUES.includes(e$1[t]) || (e$1[t] += OFFSET);
		return e$1.slice(0, n);
	}, e;
}(), BASE_PATH$1 = "https://storage.googleapis.com/tfjs-models/savedmodel/universal_sentence_encoder";
function load(e) {
	return __awaiter(this, void 0, void 0, function() {
		var n;
		return __generator(this, function(t) {
			switch (t.label) {
				case 0: return [4, (n = new UniversalSentenceEncoder()).load(e)];
				case 1: return t.sent(), [2, n];
			}
		});
	});
}
var UniversalSentenceEncoder = function() {
	function e() {}
	return e.prototype.loadModel = function(e$1) {
		return __awaiter(this, void 0, void 0, function() {
			return __generator(this, function(n) {
				return [2, e$1 ? loadGraphModel(e$1) : loadGraphModel("https://tfhub.dev/tensorflow/tfjs-model/universal-sentence-encoder-lite/1/default/1", { fromTFHub: !0 })];
			});
		});
	}, e.prototype.load = function(e$1) {
		return void 0 === e$1 && (e$1 = {}), __awaiter(this, void 0, void 0, function() {
			var n, t, r;
			return __generator(this, function(o) {
				switch (o.label) {
					case 0: return [4, Promise.all([this.loadModel(e$1.modelUrl), loadVocabulary(e$1.vocabUrl || BASE_PATH$1 + "/vocab.json")])];
					case 1: return n = o.sent(), t = n[0], r = n[1], this.model = t, this.tokenizer = new Tokenizer(r), [2];
				}
			});
		});
	}, e.prototype.embed = function(e$1) {
		return __awaiter(this, void 0, void 0, function() {
			var n, t, r, o, i, a, s, u, l = this;
			return __generator(this, function(c) {
				switch (c.label) {
					case 0:
						for ("string" == typeof e$1 && (e$1 = [e$1]), n = e$1.map(function(e$2) {
							return l.tokenizer.encode(e$2);
						}), t = n.map(function(e$2, n$1) {
							return e$2.map(function(e$3, t$1) {
								return [n$1, t$1];
							});
						}), r = [], o = 0; o < t.length; o++) r = r.concat(t[o]);
						return i = tensor2d(r, [r.length, 2], "int32"), a = tensor1d(flatten(n), "int32"), s = {
							indices: i,
							values: a
						}, [4, this.model.executeAsync(s)];
					case 1: return u = c.sent(), i.dispose(), a.dispose(), [2, u];
				}
			});
		});
	}, e;
}();

//#endregion
export { Tokenizer, UniversalSentenceEncoder, load, loadQnA, loadTokenizer, version };
//# sourceMappingURL=@tensorflow-models_universal-sentence-encoder.js.map