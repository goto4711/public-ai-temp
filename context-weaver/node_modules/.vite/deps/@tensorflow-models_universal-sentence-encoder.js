import {
  loadGraphModel,
  tensor1d,
  tensor2d,
  tidy,
  util_exports
} from "./chunk-JRY4DLUR.js";
import "./chunk-G3PMV62Z.js";

// node_modules/@tensorflow-models/universal-sentence-encoder/dist/universal-sentence-encoder.esm.js
function __awaiter(e, n, t, r) {
  return new (t || (t = Promise))(function(o, i) {
    function a(e2) {
      try {
        u(r.next(e2));
      } catch (e3) {
        i(e3);
      }
    }
    function s(e2) {
      try {
        u(r.throw(e2));
      } catch (e3) {
        i(e3);
      }
    }
    function u(e2) {
      var n2;
      e2.done ? o(e2.value) : (n2 = e2.value, n2 instanceof t ? n2 : new t(function(e3) {
        e3(n2);
      })).then(a, s);
    }
    u((r = r.apply(e, n || [])).next());
  });
}
function __generator(e, n) {
  var t, r, o, i, a = { label: 0, sent: function() {
    if (1 & o[0]) throw o[1];
    return o[1];
  }, trys: [], ops: [] };
  return i = { next: s(0), throw: s(1), return: s(2) }, "function" == typeof Symbol && (i[Symbol.iterator] = function() {
    return this;
  }), i;
  function s(i2) {
    return function(s2) {
      return (function(i3) {
        if (t) throw new TypeError("Generator is already executing.");
        for (; a; ) try {
          if (t = 1, r && (o = 2 & i3[0] ? r.return : i3[0] ? r.throw || ((o = r.return) && o.call(r), 0) : r.next) && !(o = o.call(r, i3[1])).done) return o;
          switch (r = 0, o && (i3 = [2 & i3[0], o.value]), i3[0]) {
            case 0:
            case 1:
              o = i3;
              break;
            case 4:
              return a.label++, { value: i3[1], done: false };
            case 5:
              a.label++, r = i3[1], i3 = [0];
              continue;
            case 7:
              i3 = a.ops.pop(), a.trys.pop();
              continue;
            default:
              if (!(o = (o = a.trys).length > 0 && o[o.length - 1]) && (6 === i3[0] || 2 === i3[0])) {
                a = 0;
                continue;
              }
              if (3 === i3[0] && (!o || i3[1] > o[0] && i3[1] < o[3])) {
                a.label = i3[1];
                break;
              }
              if (6 === i3[0] && a.label < o[1]) {
                a.label = o[1], o = i3;
                break;
              }
              if (o && a.label < o[2]) {
                a.label = o[2], a.ops.push(i3);
                break;
              }
              o[2] && a.ops.pop(), a.trys.pop();
              continue;
          }
          i3 = n.call(e, a);
        } catch (e2) {
          i3 = [6, e2], r = 0;
        } finally {
          t = o = 0;
        }
        if (5 & i3[0]) throw i3[1];
        return { value: i3[0] ? i3[1] : void 0, done: true };
      })([i2, s2]);
    };
  }
}
var stringToChars = function(e) {
  for (var n = [], t = 0, r = e; t < r.length; t++) {
    var o = r[t];
    n.push(o);
  }
  return n;
};
var TrieNode = /* @__PURE__ */ (function() {
  return function() {
    this.parent = null, this.children = {}, this.end = false, this.word = [[], 0, 0];
  };
})();
var Trie = (function() {
  function e() {
    this.root = new TrieNode();
  }
  return e.prototype.insert = function(e2, n, t) {
    for (var r = this.root, o = stringToChars(e2), i = 0; i < o.length; i++) r.children[o[i]] || (r.children[o[i]] = new TrieNode(), r.children[o[i]].parent = r, r.children[o[i]].word[0] = r.word[0].concat(o[i])), r = r.children[o[i]], i === o.length - 1 && (r.end = true, r.word[1] = n, r.word[2] = t);
  }, e.prototype.commonPrefixSearch = function(e2) {
    for (var n = [], t = this.root.children[e2[0]], r = 0; r < e2.length && t; r++) t.end && n.push(t.word), t = t.children[e2[r + 1]];
    return n.length || n.push([[e2[0]], 0, 0]), n;
  }, e;
})();
var separator = "â–";
function processInput(e) {
  var n = e.normalize("NFKC");
  return n.length > 0 ? separator + n.replace(/ /g, separator) : n;
}
var RESERVED_SYMBOLS_COUNT = 6;
var Tokenizer = (function() {
  function e(e2, n) {
    void 0 === n && (n = RESERVED_SYMBOLS_COUNT), this.vocabulary = e2, this.reservedSymbolsCount = n, this.trie = new Trie();
    for (var t = this.reservedSymbolsCount; t < this.vocabulary.length; t++) this.trie.insert(this.vocabulary[t][0], this.vocabulary[t][1], t);
  }
  return e.prototype.encode = function(e2) {
    var n = [], t = [], r = [];
    e2 = processInput(e2);
    for (var o = stringToChars(e2), i = 0; i <= o.length; i++) n.push({}), t.push(0), r.push(0);
    for (i = 0; i < o.length; i++) for (var a = this.trie.commonPrefixSearch(o.slice(i)), s = 0; s < a.length; s++) {
      var u = a[s], l = { key: u[0], score: u[1], index: u[2] };
      null == n[i + (c = u[0].length)][i] && (n[i + c][i] = []), n[i + c][i].push(l);
    }
    for (var c = 0; c <= o.length; c++) for (var h in n[c]) {
      var d = n[c][h];
      for (s = 0; s < d.length; s++) {
        var f = d[s], _ = f.score + r[c - f.key.length];
        (0 === r[c] || _ >= r[c]) && (r[c] = _, t[c] = d[s].index);
      }
    }
    for (var v = [], p = t.length - 1; p > 0; ) v.push(t[p]), p -= this.vocabulary[t[p]][0].length;
    var E = [], T = false;
    for (i = 0; i < v.length; i++) {
      var g = v[i];
      T && 0 === g || E.push(g), T = 0 === g;
    }
    return E.reverse();
  }, e;
})();
function loadTokenizer(e) {
  return __awaiter(this, void 0, void 0, function() {
    var n;
    return __generator(this, function(t) {
      switch (t.label) {
        case 0:
          return [4, loadVocabulary(e)];
        case 1:
          return n = t.sent(), [2, new Tokenizer(n)];
      }
    });
  });
}
function loadVocabulary(e) {
  return __awaiter(this, void 0, void 0, function() {
    return __generator(this, function(n) {
      switch (n.label) {
        case 0:
          return [4, util_exports.fetch(e)];
        case 1:
          return [2, n.sent().json()];
      }
    });
  });
}
var version = "1.3.3";
var BASE_PATH = "https://tfhub.dev/google/tfjs-model/universal-sentence-encoder-qa-ondevice/1";
var SKIP_VALUES = [0, 1, 2];
var OFFSET = 3;
var INPUT_LIMIT = 192;
var QUERY_NODE_NAME = "input_inp_text";
var RESPONSE_CONTEXT_NODE_NAME = "input_res_context";
var RESPONSE_NODE_NAME = "input_res_text";
var RESPONSE_RESULT_NODE_NAME = "Final/EncodeResult/mul";
var QUERY_RESULT_NODE_NAME = "Final/EncodeQuery/mul";
var RESERVED_SYMBOLS_COUNT$1 = 3;
var TOKEN_PADDING = 2;
var TOKEN_START_VALUE = 1;
function loadQnA() {
  return __awaiter(this, void 0, void 0, function() {
    var e;
    return __generator(this, function(n) {
      switch (n.label) {
        case 0:
          return [4, (e = new UniversalSentenceEncoderQnA()).load()];
        case 1:
          return n.sent(), [2, e];
      }
    });
  });
}
var UniversalSentenceEncoderQnA = (function() {
  function e() {
  }
  return e.prototype.loadModel = function() {
    return __awaiter(this, void 0, void 0, function() {
      return __generator(this, function(e2) {
        return [2, loadGraphModel(BASE_PATH, { fromTFHub: true })];
      });
    });
  }, e.prototype.load = function() {
    return __awaiter(this, void 0, void 0, function() {
      var e2, n, t;
      return __generator(this, function(r) {
        switch (r.label) {
          case 0:
            return [4, Promise.all([this.loadModel(), loadVocabulary(BASE_PATH + "/vocab.json?tfjs-format=file")])];
          case 1:
            return e2 = r.sent(), n = e2[0], t = e2[1], this.model = n, this.tokenizer = new Tokenizer(t, RESERVED_SYMBOLS_COUNT$1), [2];
        }
      });
    });
  }, e.prototype.embed = function(e2) {
    var n = this, t = tidy(function() {
      var t2 = n.tokenizeStrings(e2.queries, INPUT_LIMIT), r = n.tokenizeStrings(e2.responses, INPUT_LIMIT);
      if (null != e2.contexts && e2.contexts.length !== e2.responses.length) throw new Error("The length of response strings and context strings need to match.");
      var o = e2.contexts || [];
      null == e2.contexts && (o.length = e2.responses.length, o.fill(""));
      var i = n.tokenizeStrings(o, INPUT_LIMIT), a = {};
      return a[QUERY_NODE_NAME] = t2, a[RESPONSE_NODE_NAME] = r, a[RESPONSE_CONTEXT_NODE_NAME] = i, n.model.execute(a, [QUERY_RESULT_NODE_NAME, RESPONSE_RESULT_NODE_NAME]);
    });
    return { queryEmbedding: t[0], responseEmbedding: t[1] };
  }, e.prototype.tokenizeStrings = function(e2, n) {
    var t = this, r = e2.map(function(e3) {
      return t.shiftTokens(t.tokenizer.encode(e3), INPUT_LIMIT);
    });
    return tensor2d(r, [e2.length, INPUT_LIMIT], "int32");
  }, e.prototype.shiftTokens = function(e2, n) {
    e2.unshift(TOKEN_START_VALUE);
    for (var t = 0; t < n; t++) t >= e2.length ? e2[t] = TOKEN_PADDING : SKIP_VALUES.includes(e2[t]) || (e2[t] += OFFSET);
    return e2.slice(0, n);
  }, e;
})();
var BASE_PATH$1 = "https://storage.googleapis.com/tfjs-models/savedmodel/universal_sentence_encoder";
function load(e) {
  return __awaiter(this, void 0, void 0, function() {
    var n;
    return __generator(this, function(t) {
      switch (t.label) {
        case 0:
          return [4, (n = new UniversalSentenceEncoder()).load(e)];
        case 1:
          return t.sent(), [2, n];
      }
    });
  });
}
var UniversalSentenceEncoder = (function() {
  function e() {
  }
  return e.prototype.loadModel = function(e2) {
    return __awaiter(this, void 0, void 0, function() {
      return __generator(this, function(n) {
        return [2, e2 ? loadGraphModel(e2) : loadGraphModel("https://tfhub.dev/tensorflow/tfjs-model/universal-sentence-encoder-lite/1/default/1", { fromTFHub: true })];
      });
    });
  }, e.prototype.load = function(e2) {
    return void 0 === e2 && (e2 = {}), __awaiter(this, void 0, void 0, function() {
      var n, t, r;
      return __generator(this, function(o) {
        switch (o.label) {
          case 0:
            return [4, Promise.all([this.loadModel(e2.modelUrl), loadVocabulary(e2.vocabUrl || BASE_PATH$1 + "/vocab.json")])];
          case 1:
            return n = o.sent(), t = n[0], r = n[1], this.model = t, this.tokenizer = new Tokenizer(r), [2];
        }
      });
    });
  }, e.prototype.embed = function(e2) {
    return __awaiter(this, void 0, void 0, function() {
      var n, t, r, o, i, a, s, u, l = this;
      return __generator(this, function(c) {
        switch (c.label) {
          case 0:
            for ("string" == typeof e2 && (e2 = [e2]), n = e2.map(function(e3) {
              return l.tokenizer.encode(e3);
            }), t = n.map(function(e3, n2) {
              return e3.map(function(e4, t2) {
                return [n2, t2];
              });
            }), r = [], o = 0; o < t.length; o++) r = r.concat(t[o]);
            return i = tensor2d(r, [r.length, 2], "int32"), a = tensor1d(util_exports.flatten(n), "int32"), s = { indices: i, values: a }, [4, this.model.executeAsync(s)];
          case 1:
            return u = c.sent(), i.dispose(), a.dispose(), [2, u];
        }
      });
    });
  }, e;
})();
export {
  Tokenizer,
  UniversalSentenceEncoder,
  load,
  loadQnA,
  loadTokenizer,
  version
};
/*! Bundled license information:

@tensorflow-models/universal-sentence-encoder/dist/universal-sentence-encoder.esm.js:
  (**
      * @license
      * Copyright 2021 Google LLC. All Rights Reserved.
      * Licensed under the Apache License, Version 2.0 (the "License");
      * you may not use this file except in compliance with the License.
      * You may obtain a copy of the License at
      *
      * http://www.apache.org/licenses/LICENSE-2.0
      *
      * Unless required by applicable law or agreed to in writing, software
      * distributed under the License is distributed on an "AS IS" BASIS,
      * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
      * See the License for the specific language governing permissions and
      * limitations under the License.
      * =============================================================================
      *)
*/
//# sourceMappingURL=@tensorflow-models_universal-sentence-encoder.js.map
